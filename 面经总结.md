## 机器学习

### 机器学习的性能评估指标

#### 错误率，精度

###### 错误率: 分类错误的样本数占总样本的比例

$$
    Error = \frac{1}{m}\sum_{i=1}^msign(f(x_i)!=y_i)
$$

##### 精度：分类正确的样本占总样本的比例

$$
    Acc = \frac{1}{m}\sum_{i=1}^msign(f(x_i)==y_i)
$$

##### 缺点：对于类别不平衡的数据集，不能体现出模型对占比小的类别的判断能力。



#### 混淆矩阵，查准率，查全率，F1

##### 混淆矩阵：将样例的**真实类别**和学习器的**预测类别**组合划分为TP，FP，TN，FN

记忆方法：TF表示预测结果的正确与错误，PN表示预测结果的正例和反例

|          | 预测结果 |      |
| -------- | -------- | :--- |
| 真实情况 | 正例     | 反例 |
| 正例     | TP       | FN   |
| 反例     | FP       | TN   |




##### 查准率:在学习器预测为正例的情况下，正确的概率

$$
P = \frac{TP}{TP+FP}
$$

##### 查全率: 在所有正样本中，模型判断为正样本的概率

$$
    R = \frac{TP}{TP+FN}
$$

##### PR曲线：调整二分类的判断阈值，获得一个混淆矩阵，及相关查准率和查全率。

用途：

1. 选择最好的分类阈值
2. 判断模型的优劣，越外面越好，说明区分度非常大

##### F1值:查准率和查全率的调和平均
$$
    F1 = \frac{2*P*R}{P+R}
$$

##### 多分类的问题

1. Marco: 把n分类问题看成n个二分类问题计算出各自的PR, (P1, R1),(P2, R2),...,(Pn, Rn),直接对$P_i,R_i$进行平均
2. Micro: 把n分类问题看成n个二分类问题计算出各自 $(TP_i,FP_i,TN_i,FN_i)$的值再做平均


##### ROC和AUC

ROC类似PR曲线,都是使用不同的阈值来计算特征值后进行描点画图，但ROC的的特征值为TPR和FPR

$$
    TPR = \frac{TP}{TP +  FN} \\
    FPR = \frac{FP}{TN + FP}
$$

整两个特征的含义：
1. TPR: 在实际为正样本的结果中，判断正确的比例
2. FPR: 在实际为负样本的结果，判断正的比率

AUC则是ROC曲线和坐标轴所夹的面积

最后说说AUC的优势，AUC的计算方法同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。例如在反欺诈场景，设欺诈类样本为正例，正例占比很少（假设0.1%），如果使用准确率评估，把所有的样本预测为负例，便可以获得99.9%的准确率。但是如果使用AUC，把所有样本预测为负例，TPRate和FPRate同时为0（没有Positive），与(0,0) (1,1)连接，得出AUC仅为0.5，成功规避了样本不均匀带来的问题。

### 决策树，GBDT，XGBooost

#### 决策树

##### 定义：决策树是一种基本的分类和回归方法

- 决策树由节点和有向边构成，节点可以分为叶节点和非叶节点，叶节点表示一个类，非叶节点表示一个属性或特征。可以将决策树看成一个if-then的规则集合，从根节点出发，到叶节点的每一条路径可以构成一条规则。

- 从概率空间上，一般决策树的分类面多为一个阶梯状的分类面


##### 决策树生成通用流程

决策树的生成过程主要为贪婪方法: 选择让分类纯度上升最大的特征以及该特征的分类面->分类-> 开始下一个节点，

选择特征和特征分类面主要依据选择该操作后分类结果的纯度，可以使用 信息增益(ID3), 信息增益比(C4.5), Geni系数(CART)


##### ID3, C4.5, CART生成决策树

###### ID3:

决策树在各个节点上应用信息增益准则选择特征，递归地构建决策树。从根节点开始，计算对所有可能的特征的**信息增益**(意味着只能针对离散取值的特征变量)，选择最大的特征作为节点的特征，有该**特征的不同取值**(意味着只能针对离散取值的特征变量)建立节点;再对子节点不断递归调用构建决策树, 直到所有特征的信息增益均很小，或者没有特征可以选择。背后的实际上是用极大似然估计进行概率模型的选择。

###### C4.5

生成流程与ID3类似，但是用的信息增益比来选择特征，这样可以缓解偏向选择去数量较多的特征。

$$
    g(D, A) = H(D) - H(D|A) \\
    = -\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|} - (-\sum_{i=1}^n{\frac{|D_i|}{|D|}}\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|})
$$

连续变量的处理方法：

二分法，将所有该特征的取值排序，穷举每个数据之间的均值进行划分点，计算


**剪枝**

剪枝的通过及消化决策树的整体损失函数(带有正则项(结构风险))的损失函数来评估，自底而上的进行

###### CART

可以用于分类和回归的决策树，依旧是特征选择 -> 树的生成 -> 剪枝。

回归树用平方误差最小，分类树用基尼指数。

基尼系数，假设有K个类，样本点是k的概率为$p_k$，则概率分布的基尼系数为

$$
Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2
$$

**CART剪枝**

$$
    C_\alpha(t) = C(t) + \alpha \\
    C_\alpha(T_t) = C(T_t) + \alpha |T_t|           两者做差\\
    g(t) = \alpha = \frac{C(t)-C(T_t)}{|T_t|-1} 
$$

$$
    g(t) = \frac{C(t)-C(T_t)}{|T_t|-1} \\ 
$$

在原有剪枝的基础上进行剪枝，然后每做一个操作形成一个子树，将子树序列保存，利用交叉验证的方法，在肚里的验证数据集上进行测试，选择最优的子树。

### Bagging与Boosting



### 如何解决过拟合


### 数据不平衡问题

模型层面：

数据层面：
1. 欠采样
2. 过采样
3. 阈值移动（代价敏感学习）






## 深度学习

### 1. 参数初始化问题




### 2. word2vec以及维度的确定





### 3.  LSTM和梯度爆炸和弥散

#### 一切从RNN的梯度爆炸和梯度弥散开始说起

RNN公式

$$
    h_t = f(Uh_{t-1} + Wx_t + b) \\
    y_t = Vh_t
$$

RNN好吗？好啊，通用近似定理保证了可以拟合任意的有界笔记上的任意连续函数；而且是图灵完备的(Transformers不是图灵完备的)


##### 梯度的爆炸与弥散

链式法则：复合函数求导，等于各个函数的导数相乘

当序列长度非常长的时候，导数的连乘会非常大或非常小，非常大导致模型不稳定（NaN），梯度非常小，导致前面的信息无法传导到后面。

由 1 中所述的原因，**RNN 中总的梯度是不会消失的**。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。**RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。**

#### LSTM的改进方案
$$
\left[\begin{array}{c}
\tilde{c}_{t} \\
\boldsymbol{o}_{t} \\
\boldsymbol{i}_{t} \\
\boldsymbol{f}_{t}
\end{array}\right]=\left[\begin{array}{c}
\tanh \\
\sigma \\
\sigma \\
\sigma
\end{array}\right]\left(\boldsymbol{W}\left[\begin{array}{c}
\boldsymbol{x}_{t} \\
\boldsymbol{h}_{t-1}
\end{array}\right]+\boldsymbol{b}\right)\\

\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t} \\

\boldsymbol{h}_{t}=\boldsymbol{o}_{t} \odot tanh({\boldsymbol{c}}_{t})
$$

在神经网络中，长期记忆 （ Long-Term Memory）可以看作是网络参数，隐含了从训练数据中学到的经验，其更新周期要远远慢于短期记忆。

而在 LSTM 网络中，记忆单元 c 可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔。记忆单元 c中保存信息的生命周期要长于短期记忆 h，但又远远短于长期记忆

从反向回传的角度来说，$\boldsymbol{c}_{t}=\boldsymbol{f}_{t} \odot \boldsymbol{c}_{t-1}+\boldsymbol{i}_{t} \odot \tilde{\boldsymbol{c}}_{t}$这条道路是没有复合函数的，只有加和乘，c可以快速的回传回去，其他的渠道还是需要多次计算

### 4. Transformer原理和Position Embedding



### 5. L1和L2的正则化